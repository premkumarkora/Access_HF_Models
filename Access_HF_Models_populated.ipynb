{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B70YCyKV2BpC"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    print(f\"HF_TOKEN: {hf_token}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching user data: {e}\")\n",
        "    print(\"Checking if running in Colab...\")\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        print(\"Running in Colab. Check if the notebook is running interactively.\")\n",
        "    else:\n",
        "        print(\"Not running in Colab. User data functions are not available.\")\n",
        "\n",
        "# If not running in Colab or userdata fetch failed:\n",
        "#   - Provide an alternative way to retrieve the token\n",
        "#   - For instance, load from a local file or environment variable\n",
        "# Example:\n",
        "# hf_token = os.environ.get('HF_TOKEN')  # Get token from environment variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cd615d"
      },
      "source": [
        "This cell specifies the prompt for generating code that retrieves and displays real-time usage statistics for all system resources (CPU, memory, disk, and GPU) in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIH2MYSX3Sa-"
      },
      "outputs": [],
      "source": [
        "# prompt: WRITE CODE TO get the usage of all the system resources that I am using\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    print(f\"HF_TOKEN: {hf_token}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching user data: {e}\")\n",
        "    print(\"Checking if running in Colab...\")\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        print(\"Running in Colab. Check if the notebook is running interactively.\")\n",
        "    else:\n",
        "        print(\"Not running in Colab. User data functions are not available.\")\n",
        "\n",
        "# Get system resource usage\n",
        "def get_system_resource_usage():\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)  # CPU usage percentage\n",
        "    memory = psutil.virtual_memory()\n",
        "    memory_percent = memory.percent  # Memory usage percentage\n",
        "    disk = psutil.disk_usage('/')\n",
        "    disk_percent = disk.percent  # Disk usage percentage\n",
        "\n",
        "    print(f\"CPU Usage: {cpu_percent}%\")\n",
        "    print(f\"Memory Usage: {memory_percent}%\")\n",
        "    print(f\"Disk Usage: {disk_percent}%\")\n",
        "\n",
        "\n",
        "get_system_resource_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89B37Sxs3qub"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzF6p_uf6Qsy"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJgSi7OP6kaC"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reFAi9ze-tIN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT6RZQPd7Nt5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fe7f47"
      },
      "source": [
        "In this section, we perform sentiment analysis using a pretrained transformer model via Hugging Face's pipeline. The code initializes a sentiment-analysis pipeline on the GPU and applies it to an example sentence, outputting whether the sentiment is positive or negative along with confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vysjJIYD-02-"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"I'm  super excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24690709"
      },
      "source": [
        "This section sets up a named entity recognition (NER) pipeline using a pretrained transformer model. It groups recognized entities and processes a sample sentence to extract and display entities such as names, locations, and organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXr-9XHV_utW"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"PremKumar Kora is a Data Scientist from India\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8dae3a0"
      },
      "source": [
        "Here, we use a question-answering pipeline by providing both a question and context. The code loads the model on the GPU, runs inference to find the answer span within the provided context, and outputs the answer with a confidence score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riDKetdeANQA"
      },
      "outputs": [],
      "source": [
        "# Question Answering with Context\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d15f10f"
      },
      "source": [
        "This section demonstrates text summarization using a transformer-based summarization pipeline. It processes a longer text input and outputs a concise summary that captures the key points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0zhj5lvAhqs"
      },
      "outputs": [],
      "source": [
        "# Text Summarization\n",
        "\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "390c3fde"
      },
      "source": [
        "In this cell, we perform machine translation using a pretrained translation pipeline. The code translates input text from one language to another and prints the translated output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0pmtS4cBQ-w"
      },
      "outputs": [],
      "source": [
        "# Translation\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340bdda0"
      },
      "source": [
        "This example illustrates how to specify a particular translation model by name when creating the pipeline. It translates text using the explicitly defined model, showing how to override the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxGeP58qCVh4"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc1d143b"
      },
      "source": [
        "This section showcases text classification beyond sentiment analysis. The code loads a classification pipeline, applies it to input text, and prints the predicted class labels along with their scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YnkNNSgCdt7"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a8ed2be"
      },
      "source": [
        "Here, we generate new text using a language generation pipeline. The cell loads a text-generation model, provides a prompt, and outputs generated continuations or completions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0To9cYmCqa5"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e826770e"
      },
      "source": [
        "This part of the notebook uses an image generation pipeline (e.g., Stable Diffusion) to create images from text prompts. The code loads the model, generates an image based on a prompt, and displays or saves the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkXiawVAC3aA"
      },
      "outputs": [],
      "source": [
        "# Image Generation\n",
        "\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"give image of Madurai Meenachi Temple in sketch style\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71c304a9"
      },
      "source": [
        "In this section, we use an audio generation pipeline to synthesize audio from text or other inputs. The code loads the model, generates audio samples, and outputs them for playback or saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUye_4ALDbix"
      },
      "outputs": [],
      "source": [
        "# Audio Generation\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hi, I would like to introduce PremKumar Kora, A renowned Data Scientist\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9dc3e1"
      },
      "source": [
        "This cell provides background on the MusicLDM model, which was trained on 466 hours of music data. It explains the training dataset size and context before using the model for music generation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks3xNof2E8Cc"
      },
      "outputs": [],
      "source": [
        "# MusicLDM is trained on a corpus of 466 hours of music data.\n",
        "# Beat-synchronous data augmentation strategies are applied to the music samples, both in the\n",
        "# time domain and in the latent space. Using beat-synchronous data augmentation strategies\n",
        "# encourages the model to interpolate between the training samples, but stay within the domain\n",
        "# of the training data. The result is generated music that is more diverse while staying faithful\n",
        "# to the corresponding style.\n",
        "\n",
        "from diffusers import MusicLDMPipeline\n",
        "import torch\n",
        "import scipy\n",
        "\n",
        "repo_id = \"ucsd-reach/musicldm\"\n",
        "pipe = MusicLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"heavy drums mixed with gutar\"\n",
        "audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=15.0).audios[0]\n",
        "\n",
        "# save the audio sample as a .wav file\n",
        "scipy.io.wavfile.write(\"flute.wav\", rate=16000, data=audio)\n",
        "Audio(\"flute.wav\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}